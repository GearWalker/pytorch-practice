{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.Field(unk_token=None, pad_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f4e55d13748>, 'label': <torchtext.data.field.Field object at 0x7f4ddcead400>}\n",
      "len(train) 25000\n",
      "vars(train[0]) {'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': ['pos']}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 280619\n",
      "TEXT.vocab.vectors.size() torch.Size([280619, 300])\n"
     ]
    }
   ],
   "source": [
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(LABEL.vocab) 2\n",
      "defaultdict(<function _default_unk_index at 0x7f4e60207488>, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print('len(LABEL.vocab)', len(LABEL.vocab))\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, test), batch_size=3, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.3300e+02  1.1103e+05  1.8374e+04\n",
      " 3.4000e+02  7.0000e+00  6.0000e+00\n",
      " 6.9440e+03  3.2000e+01  3.4689e+04\n",
      " 7.7310e+03  5.0000e+00  7.0000e+00\n",
      " 2.8370e+03  1.5100e+02  3.2000e+01\n",
      " 9.0700e+02  1.3600e+02  5.0000e+00\n",
      " 2.0300e+02  1.0000e+01  2.0000e+00\n",
      " 8.0000e+00  2.0000e+00  8.1000e+01\n",
      " 2.0000e+00  3.5820e+03  2.1980e+03\n",
      " 2.4000e+01  1.1820e+05  1.3600e+02\n",
      " 1.7400e+02  2.1200e+02  9.0000e+00\n",
      " 4.8000e+01  2.5000e+01  2.5000e+01\n",
      " 4.3000e+01  7.2000e+01  1.3000e+02\n",
      " 3.4100e+02  6.2230e+03  7.2700e+02\n",
      " 3.6000e+01  1.7900e+03  1.3300e+02\n",
      " 2.4985e+04  1.6740e+03  1.1370e+03\n",
      " 1.8000e+01  1.0200e+02  3.5290e+03\n",
      " 1.0400e+02  2.3800e+03  1.2820e+03\n",
      " 7.3000e+01  4.0000e+00  3.6480e+03\n",
      " 2.2000e+01  5.3080e+03  1.0990e+03\n",
      " 4.2370e+03  1.0460e+03  4.0000e+00\n",
      " 2.0300e+02  8.3680e+03  4.3300e+02\n",
      " 4.1000e+01  1.0200e+02  3.1810e+03\n",
      " 1.2980e+03  2.9900e+02  1.3300e+02\n",
      " 1.4630e+03  8.8000e+01  1.4118e+04\n",
      " 3.6000e+01  8.2000e+01  2.1000e+01\n",
      " 2.0000e+00  6.7000e+01  1.0000e+01\n",
      " 7.2090e+04  1.0229e+04  1.4700e+02\n",
      " 1.0142e+04  1.4420e+03  7.5000e+01\n",
      " 9.4850e+03  2.1500e+02  2.3986e+04\n",
      " 1.1300e+02  2.0000e+00  2.1500e+02\n",
      " 1.0000e+01  1.0877e+04  8.0000e+00\n",
      " 2.9891e+04  1.5650e+03  2.0000e+00\n",
      " 1.9000e+01  7.9000e+01  1.4710e+03\n",
      " 2.2800e+02  2.9000e+01  4.9032e+04\n",
      " 4.1300e+02  6.4000e+01  1.3000e+01\n",
      " 6.0000e+00  7.8900e+02  2.4010e+03\n",
      " 2.7000e+01  2.9800e+02  7.8000e+01\n",
      " 8.0000e+00  3.7300e+02  6.4000e+01\n",
      " 1.2900e+02  2.0000e+00  2.7000e+01\n",
      " 1.6000e+01  2.5018e+05  3.5500e+02\n",
      " 9.8570e+03  5.6000e+01  4.2000e+01\n",
      " 3.3249e+04  7.0000e+00  1.1000e+01\n",
      " 1.9000e+01  2.3459e+04  2.4000e+01\n",
      " 5.4000e+01  1.3047e+04  2.0600e+02\n",
      " 5.9000e+01  3.1000e+01  9.3360e+03\n",
      " 3.3300e+03  2.0000e+00  1.1400e+02\n",
      " 2.7800e+02  1.7800e+02  1.9000e+01\n",
      " 1.4000e+01  3.0000e+01  6.6880e+03\n",
      " 2.0000e+00  3.0000e+00  4.5700e+02\n",
      " 3.0700e+02  2.0490e+03  5.8600e+02\n",
      " 3.5000e+01  8.6730e+03  1.2000e+01\n",
      " 2.6600e+02  2.5780e+03  3.7000e+01\n",
      " 6.6202e+04  3.5410e+03  7.1000e+01\n",
      " 1.8000e+01  7.0000e+00  8.5800e+02\n",
      " 2.4600e+02  1.5160e+03  4.7710e+03\n",
      " 7.3000e+01  3.0000e+01  8.2600e+02\n",
      " 1.4000e+01  3.0000e+00  8.9357e+04\n",
      " 6.8000e+01  3.5600e+02  5.0400e+02\n",
      " 2.1400e+02  1.2880e+03  1.4447e+05\n",
      " 5.6426e+04  1.0000e+01  1.3000e+01\n",
      " 9.0000e+00  3.1850e+03  1.8980e+04\n",
      " 9.6000e+01  5.4000e+01  8.3880e+03\n",
      " 1.2200e+02  8.0000e+00  7.5410e+03\n",
      " 6.2000e+01  1.1950e+03  4.0000e+00\n",
      " 6.8500e+02  3.0000e+00  1.3760e+03\n",
      " 1.0000e+01  9.9000e+01  1.9187e+04\n",
      " 1.3750e+03  1.8160e+03  4.9300e+02\n",
      " 4.9000e+01  8.0000e+00  3.3000e+01\n",
      " 3.2000e+01  2.5900e+02  5.7900e+02\n",
      " 7.0000e+00  3.0523e+04  1.6120e+03\n",
      " 2.8500e+02  3.1900e+02  7.7400e+02\n",
      " 7.1500e+03  3.0000e+00  9.5800e+03\n",
      " 5.0590e+03  4.7000e+01  1.3500e+02\n",
      " 7.5310e+03  9.1500e+02  2.7000e+01\n",
      " 1.0000e+00  1.4400e+02  2.9973e+04\n",
      " 1.0000e+00  1.0000e+00  1.7000e+01\n",
      " 1.0000e+00  1.0000e+00  4.4550e+03\n",
      " 1.0000e+00  1.0000e+00  1.1000e+01\n",
      " 1.0000e+00  1.0000e+00  2.7916e+04\n",
      " 1.0000e+00  1.0000e+00  1.3000e+01\n",
      " 1.0000e+00  1.0000e+00  7.3000e+02\n",
      "[torch.LongTensor of size 82x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(batch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  2  3\n",
      "[torch.LongTensor of size 1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
