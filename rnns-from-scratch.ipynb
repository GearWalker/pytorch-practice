{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/10/17 - Training RNNs as Fast as CNNs\n",
    "\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://arxiv.org/abs/1709.02755\n",
    "- https://github.com/taolei87/sru\n",
    "- https://arxiv.org/abs/1611.01576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of standard RNN from scratch\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
    "- http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
    "- https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "$x_t$ is input at time step $t$\n",
    "\n",
    "$h_t$ is hidden state at time step $t$. It is calculated via $h_t=f(Ux_t+Wh_{t-1})$, where $f$ is a non-linearity. $h_{0}$ (the initial hidden state) is typically initialised to all zeros.\n",
    "  \n",
    "$y_t$ is the output at step $t$, i.e. $y_t = V h_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.w_xh = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to hidden, aka U\n",
    "        self.w_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to hidden, aka W\n",
    "        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size)) #hidden to output, aka V\n",
    "        \n",
    "        self.b_h = nn.Parameter(torch.Tensor(hidden_size)) #hidden bias\n",
    "        self.b_y = nn.Parameter(torch.Tensor(input_size)) #output bias\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        \n",
    "        #h_t = tanh(w_xh * x_t + w_hh * h_(t-1) + b_h) [note, here '*' implies matrix multiplication]\n",
    "        h = F.tanh(torch.matmul(self.w_xh, x) + torch.matmul(self.w_hh, h) + self.b_h)\n",
    "        \n",
    "        #y_t = w_hy * h_t + b_y\n",
    "        y = torch.matmul(self.w_hy, h) + self.b_y\n",
    "        \n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test to see if it goes through with no error\n",
    "rnn = RNN(10, 100)\n",
    "\n",
    "x = Variable(torch.Tensor(10))\n",
    "h = Variable(torch.Tensor(100))\n",
    "\n",
    "y, h = rnn(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Gated Recurrent Unit (GRU) from scratch\n",
    "- The \"new gate\" terminology comes from http://pytorch.org/docs/master/nn.html#torch.nn.GRU\n",
    "- https://arxiv.org/pdf/1412.3555.pdf\n",
    "- https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.w_xz = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to update gate\n",
    "        self.w_hz = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to update gate\n",
    "        self.w_xr = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to reset gate\n",
    "        self.w_hr = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to reset gate\n",
    "        self.w_xn = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to new gate\n",
    "        self.w_hn = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to new gate\n",
    "        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size)) #hidden to output gate\n",
    "        \n",
    "        self.b_z = nn.Parameter(torch.Tensor(hidden_size)) #update gate bias\n",
    "        self.b_r = nn.Parameter(torch.Tensor(hidden_size)) #reset gate bias\n",
    "        self.b_n = nn.Parameter(torch.Tensor(hidden_size)) #new gate bias\n",
    "        self.b_y = nn.Parameter(torch.Tensor(input_size)) #output bia\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        \n",
    "        #z_t = sigmoid(w_xz * x_t + w_hz * h + b_z) [note, here '*' implies matrix multiplication]\n",
    "        z = F.sigmoid(torch.matmul(self.w_xz, x) + torch.matmul(self.w_hz, h) + self.b_z)\n",
    "        \n",
    "        #r_t = sigmoid(w_xr * x_t + w_hr * h + b_r)\n",
    "        r = F.sigmoid(torch.matmul(self.w_xr, x) + torch.matmul(self.w_hr, h) + self.b_r)\n",
    "        \n",
    "        #n_t = tanh(w_xn * x_t + w_hn * (r • h_(t-1)) + b_n) [note, here '•' is elementwise multiplication]\n",
    "        n = F.tanh(torch.matmul(self.w_xn, x) + torch.matmul(self.w_hn, r * h) + self.b_n)\n",
    "        \n",
    "        #h_t = (1 - z_t) • n_t + z_t • h_t\n",
    "        h = (1 - z) * n + z * h\n",
    "        \n",
    "        #y_t = w_hy * h_t + b_y\n",
    "        y = torch.matmul(self.w_hy, h) + self.b_y\n",
    "        \n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140052903005264"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru = GRU(10, 100)\n",
    "\n",
    "x = Variable(torch.Tensor(10))\n",
    "h = Variable(torch.Tensor(100))\n",
    "\n",
    "y, h = gru(x, h)\n",
    "\n",
    "params = gru.parameters()\n",
    "\n",
    "id(y)\n",
    "\n",
    "#for p in params:\n",
    " #   print(Variable(p.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Long Short-Term Memory (LSTM) from scratch\n",
    "- https://apaszke.github.io/lstm-explained.html\n",
    "- https://deeplearning4j.org/lstm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.w_xf = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to forget gate\n",
    "        self.w_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to forget gate\n",
    "        self.w_xi = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to input gate\n",
    "        self.w_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to forget gate\n",
    "        self.w_xo = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to output gate\n",
    "        self.w_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to output gate\n",
    "        self.w_xc = nn.Parameter(torch.Tensor(hidden_size, input_size)) #input to cell\n",
    "        self.w_hc = nn.Parameter(torch.Tensor(hidden_size, hidden_size)) #hidden to cell\n",
    "        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size)) #hidden to output\n",
    "                \n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size)) #forget gate bias\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size)) #input gate bias\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size)) #output gate bias\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size)) #cell bias\n",
    "        self.b_y = nn.Parameter(torch.Tensor(input_size)) #output bias\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        \n",
    "        #f_t = sigmoid(w_xf * x_t + w_hf * h_(t-1) + b_f) [note, here '*' implies matrix multiplication]\n",
    "        f = F.sigmoid(torch.matmul(self.w_xf, x) + torch.matmul(self.w_hf, h) + self.b_f)\n",
    "\n",
    "        #i_t = sigmoid(w_xi * x_t + w_hi * h_(t-1) + b_i)\n",
    "        i = F.sigmoid(torch.matmul(self.w_xi, x) + torch.matmul(self.w_hi, h) + self.b_i)\n",
    "\n",
    "        #o_t = sigmoid(w_xo * x_t + w_ho * h_(t-1) + b_o)\n",
    "        o = F.sigmoid(torch.matmul(self.w_xo, x) + torch.matmul(self.w_ho, h) + self.b_o)\n",
    "\n",
    "        #c_in = tanh(w_xc * x_t + w_hc * h_(t-1) + b_c)\n",
    "        #'c_in' is also known as 'g' in the PyTorch docs (http://pytorch.org/docs/master/nn.html#torch.nn.LSTM)\n",
    "        c_in = F.tanh(torch.matmul(self.w_xc, x) + torch.matmul(self.w_hc, h) + self.b_c) \n",
    "\n",
    "        #c_t = f_t • c_(t-1) + i_t • c_in [note, here '•' is elementwise multiplication]\n",
    "        c = f * c + i * c_in\n",
    "        \n",
    "        #h_t = o_t • tanh(c_t)\n",
    "        h = o * F.tanh(c)\n",
    "        \n",
    "        #y_t = w_hy * h_t + b_y\n",
    "        y = torch.matmul(self.w_hy, h) + self.b_y\n",
    "        \n",
    "        return y, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = LSTM(10, 100)\n",
    "\n",
    "x = Variable(torch.Tensor(10))\n",
    "h = Variable(torch.Tensor(100))\n",
    "c = Variable(torch.Tensor(100))\n",
    "\n",
    "y, h, c = lstm(x, h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Simple Recurrent Unit (SRU) from scratch\n",
    "- https://arxiv.org/pdf/1709.02755.pdf\n",
    "- https://github.com/taolei87/sru\n",
    "- https://github.com/musyoku/chainer-sru\n",
    "- https://github.com/titu1994/keras-SRU\n",
    "\n",
    "$\\tilde{x_t}=Wx$\n",
    "\n",
    "$f_t = \\sigma(W_fx_t+b_f)$\n",
    "\n",
    "$r_t = \\sigma(W_rx_t+b_r)$\n",
    "\n",
    "$c_t = f_t \\odot c_{t-1} + (1-f_t) \\odot \\tilde{x_t}$\n",
    "\n",
    "$h_t = r_t \\odot g(c_t) + (1-r_t) \\odot x_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SRU, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.w_xt = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_xf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_xr = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        \n",
    "        self.b_x = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_r = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_y = nn.Parameter(torch.Tensor(input_size))\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        \n",
    "        t = torch.matmul(self.w_xt, x)\n",
    "        \n",
    "        f = F.sigmoid(torch.matmul(self.w_xf, x) + self.b_f)\n",
    "        \n",
    "        r = F.sigmoid(torch.matmul(self.w_xr, x) + self.b_r)\n",
    "\n",
    "        c = f * c + (1 - f) * t\n",
    "\n",
    "        h = r * F.tanh(c) + (1 - r) * x\n",
    "        \n",
    "        #y_t = w_hy * h_t + b_y\n",
    "        y = torch.matmul(self.w_hy, h) + self.b_y\n",
    "        \n",
    "        return y, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** SRU usage is slightly different as input dimensions have to be the same dimensions as the hidden dimensons. \n",
    "\n",
    "This means you need an embedding layer if input dim != hidden dim, which it usually will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sru = SRU(100, 100) #input and hidden dim must be equal!\n",
    "\n",
    "x = Variable(torch.Tensor(100)) #input dim now 100, previously has been 10\n",
    "h = Variable(torch.Tensor(100))\n",
    "c = Variable(torch.Tensor(100))\n",
    "\n",
    "y, h, c = sru(x, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RE-DO BELOW WITH EMBEDDING AS SEPARATE CLASS AND FEED IN SINGLE EXAMPLE AT A TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SRUWithEmbedding(nn.Module):\\n    def __init__(self, input_size, hidden_size):\\n        super(SRUWithEmbedding, self).__init__()\\n        \\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        \\n        self.embedding = nn.Embedding(input_size, hidden_size)\\n        \\n        self.w_xt = nn.Parameter(torch.Tensor(hidden_size, input_size))\\n        self.w_xf = nn.Parameter(torch.Tensor(hidden_size, input_size))\\n        self.w_xr = nn.Parameter(torch.Tensor(hidden_size, input_size))\\n        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size))\\n        \\n        self.b_x = nn.Parameter(torch.Tensor(hidden_size))\\n        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\\n        self.b_r = nn.Parameter(torch.Tensor(hidden_size))\\n        self.b_y = nn.Parameter(torch.Tensor(input_size))\\n        \\n    def forward(self, x, h, c):\\n        \\n        x = self.embedding(x)\\n        t = torch.matmul(self.w_xt, x)\\n        \\n        f = F.sigmoid(torch.matmul(self.w_xf, x) + self.b_f)\\n        \\n        r = F.sigmoid(torch.matmul(self.w_xr, x) + self.b_r)\\n\\n        c = f * c + (1 - f) * t\\n\\n        print(\"r\",r.size())\\n        print(\"c\",c.size())\\n        print(\"x\",x.size())\\n        h = r * F.tanh(c) + (1 - r) * x\\n        \\n        #y_t = w_hy * h_t + b_y\\n        y = torch.matmul(self.w_hy, h) + self.b_y \\n        \\n        return y, h, c'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class SRUWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SRUWithEmbedding, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        self.w_xt = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_xf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_xr = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        \n",
    "        self.b_x = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_r = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_y = nn.Parameter(torch.Tensor(input_size))\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        t = torch.matmul(self.w_xt, x)\n",
    "        \n",
    "        f = F.sigmoid(torch.matmul(self.w_xf, x) + self.b_f)\n",
    "        \n",
    "        r = F.sigmoid(torch.matmul(self.w_xr, x) + self.b_r)\n",
    "\n",
    "        c = f * c + (1 - f) * t\n",
    "\n",
    "        print(\"r\",r.size())\n",
    "        print(\"c\",c.size())\n",
    "        print(\"x\",x.size())\n",
    "        h = r * F.tanh(c) + (1 - r) * x\n",
    "        \n",
    "        #y_t = w_hy * h_t + b_y\n",
    "        y = torch.matmul(self.w_hy, h) + self.b_y \n",
    "        \n",
    "        return y, h, c\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sru = SRUWithEmbedding(1, 100) #input and hidden dim must be equal!\\n\\nx = Variable(torch.ones([1]).long()) #input dim now 100, previously has been 10\\nh = Variable(torch.Tensor(100))\\nc = Variable(torch.Tensor(100))\\nprint(x)\\ny, h, c = sru(x, h, c)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sru = SRUWithEmbedding(1, 100) #input and hidden dim must be equal!\n",
    "\n",
    "x = Variable(torch.ones([1]).long()) #input dim now 100, previously has been 10\n",
    "h = Variable(torch.Tensor(100))\n",
    "c = Variable(torch.Tensor(100))\n",
    "print(x)\n",
    "y, h, c = sru(x, h, c)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementation of Quasi-Recurrent Neural Network (QRNN) from scratch\n",
    "- https://arxiv.org/abs/1611.01576\n",
    "- https://github.com/JayParks/quasi-rnn\n",
    "- https://github.com/salesforce/pytorch-qrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size):\n",
    "        super(QRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.z_conv = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size)\n",
    "        self.f_conv = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size)\n",
    "        self.o_conv = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size)\n",
    "\n",
    "        self.conv_linear = nn.Linear(hidden_size, 3*hidden_size)\n",
    "        self.rnn_linear = (2*hidden_size, hidden_size)\n",
    "        \n",
    "        self.w_xt = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_xf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_xr = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.w_hy = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
    "        \n",
    "        self.b_x = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_r = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_y = nn.Parameter(torch.Tensor(input_size))\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        \n",
    "        print(\"x\",x.size())\n",
    "        print(\"h\",h.size())\n",
    "        print(\"c\",c.size())\n",
    "        \n",
    "        #squeeze as conv layer expects size [batch_size, input_size, length]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        #CONVOLUTION STAGE\n",
    "        \n",
    "        z = F.tanh(self.z_conv(x)).squeeze(0)\n",
    "        \n",
    "        f = F.sigmoid(self.f_conv(x)).squeeze(0)\n",
    "        \n",
    "        o = F.sigmoid(self.o_conv(x)).squeeze(0)\n",
    "        \n",
    "        print(\"z\",z.size())\n",
    "        print(\"f\",z.size())\n",
    "        print(\"o\",z.size())\n",
    "        \n",
    "        #POOL STAGE\n",
    "       \n",
    "        #fo-pooling\n",
    "        \n",
    "        c_ = (f*c.unsqueeze(1)) + (1-f)*z\n",
    "        \n",
    "        h = o * c_ \n",
    "        \n",
    "        print(\"fo-pool c\",c_.size())\n",
    "        print(\"fo-pool h\",h.size())      \n",
    "        \n",
    "        \"\"\"t = torch.matmul(self.w_xt, x)\n",
    "        \n",
    "        f = F.sigmoid(torch.matmul(self.w_xf, x) + self.b_f)\n",
    "        \n",
    "        r = F.sigmoid(torch.matmul(self.w_xr, x) + self.b_r)\n",
    "\n",
    "        c = f * c + (1 - f) * t\n",
    "\n",
    "        h = r * F.tanh(c) + (1 - r) * x\n",
    "        \n",
    "        #y_t = w_hy * h_t + b_y\n",
    "        y = torch.matmul(self.w_hy, h) + self.b_y\"\"\"\n",
    "        \n",
    "        print(\"END\")\n",
    "        \n",
    "        return y, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([50, 13])\n",
      "h torch.Size([100])\n",
      "c torch.Size([100])\n",
      "z torch.Size([100, 10])\n",
      "f torch.Size([100, 10])\n",
      "o torch.Size([100, 10])\n",
      "fo-pool c torch.Size([100, 10])\n",
      "fo-pool h torch.Size([100, 10])\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "#x = T*n, where T is length of sequence and n is dimension of input\n",
    "#z = convolutions across time step, with bank of m filters producing T*m\n",
    "#filters of width k, each z_t only depends on x_{t-k-1} through x_t, done by padding to left size k-1\n",
    "qrnn = QRNN(50, 100, 4)\n",
    "\n",
    "x = Variable(torch.Tensor(50,13)) #size of 2 here as need to pad (kernel_size-1)\n",
    "h = Variable(torch.Tensor(100))\n",
    "c = Variable(torch.Tensor(100))\n",
    "\n",
    "y, h, c = qrnn(x, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
